{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4661: Introduction to Data Science\n",
    "## Jay Tandel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: Predicting Heart Disease\n",
    "In this question, we work with a dataset from the textbook of \"An Introduction to Statistical Learning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "Read the data file “Hearts_s.csv” (from github using the following command), and assign it to a Pandas DataFrame:   \n",
    "\n",
    "df = pd.read_csv(\"https://github.com/mpourhoma/CS4661/raw/master/Heart_s.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file from web, and store it in panda DataFrame\n",
    "df = pd.read_csv(\"https://github.com/mpourhoma/CS4661/raw/master/Heart_s.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "Check out the dataset. As you see, the dataset contains a number of features including both contextual and biological factors (e.g. age, gender, vital signs, …). The last column “AHD” is the label with “Yes” meaning that a human subject has Heart Disease, and “No” meaning that the subject does not have Heart Disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age Gender     ChestPain  RestBP  Chol  RestECG  MaxHR  Oldpeak  \\\n",
      "0     63      f       typical     145   233        2    150      2.3   \n",
      "1     67      f  asymptomatic     160   286        2    108      1.5   \n",
      "2     67      f  asymptomatic     120   229        2    129      2.6   \n",
      "3     37      f    nonanginal     130   250        0    187      3.5   \n",
      "4     41      m    nontypical     130   204        2    172      1.4   \n",
      "..   ...    ...           ...     ...   ...      ...    ...      ...   \n",
      "296   45      f       typical     110   264        0    132      1.2   \n",
      "297   68      f  asymptomatic     144   193        0    141      3.4   \n",
      "298   57      f  asymptomatic     130   131        0    115      1.2   \n",
      "299   57      m    nontypical     130   236        2    174      0.0   \n",
      "300   38      f    nonanginal     138   175        0    173      0.0   \n",
      "\n",
      "           Thal  AHD  \n",
      "0         fixed   No  \n",
      "1        normal  Yes  \n",
      "2    reversable  Yes  \n",
      "3        normal   No  \n",
      "4        normal   No  \n",
      "..          ...  ...  \n",
      "296  reversable  Yes  \n",
      "297  reversable  Yes  \n",
      "298  reversable  Yes  \n",
      "299      normal  Yes  \n",
      "300      normal   No  \n",
      "\n",
      "[301 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#print the imported dataset\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n",
    "As you see, there are at least 3 categorical features in the dataset (Gender, ChestPain, Thal). Let’s ignore these categorical features for now, only keep the numerical features and build your feature matrix and label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak\n",
      "0     63     145   233        2    150      2.3\n",
      "1     67     160   286        2    108      1.5\n",
      "2     67     120   229        2    129      2.6\n",
      "3     37     130   250        0    187      3.5\n",
      "4     41     130   204        2    172      1.4\n",
      "..   ...     ...   ...      ...    ...      ...\n",
      "296   45     110   264        0    132      1.2\n",
      "297   68     144   193        0    141      3.4\n",
      "298   57     130   131        0    115      1.2\n",
      "299   57     130   236        2    174      0.0\n",
      "300   38     138   175        0    173      0.0\n",
      "\n",
      "[301 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating the Feature Matrix for the dataset:\n",
    "\n",
    "# create a python list of feature names that would like to pick from the dataset:\n",
    "feature_cols = ['Age','RestBP','Chol','RestECG','MaxHR','Oldpeak']\n",
    "\n",
    "# use the above list to select the features from the original DataFrame\n",
    "X = df[feature_cols] \n",
    "\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       No\n",
      "1      Yes\n",
      "2      Yes\n",
      "3       No\n",
      "4       No\n",
      "      ... \n",
      "296    Yes\n",
      "297    Yes\n",
      "298    Yes\n",
      "299    Yes\n",
      "300     No\n",
      "Name: AHD, Length: 301, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# select label (the last column) from the DataFrame\n",
    "y = df['AHD']\n",
    "\n",
    "print(y)\n",
    "\n",
    "# Change Categorical label to numeric label\n",
    "# Replace No with 0 and Yes with 1\n",
    "# y = y.replace('No',0)\n",
    "# y = y.replace('Yes',1)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n",
    "Split the dataset into testing and training sets with the following parameters: test_size=0.25, random_state=6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly splitting the original dataset into training set and testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak\n",
      "270   46     140   311        0    120      1.8\n",
      "69    46     150   231        0    147      3.6\n",
      "289   55     132   342        0    166      1.2\n",
      "294   59     164   176        2     90      1.0\n",
      "143   58     105   240        2    154      0.6\n",
      "..   ...     ...   ...      ...    ...      ...\n",
      "1     67     160   286        2    108      1.5\n",
      "281   35     122   192        0    174      0.0\n",
      "106   57     128   229        2    150      0.4\n",
      "227   54     110   206        2    108      0.0\n",
      "201   57     150   126        0    173      0.2\n",
      "\n",
      "[225 rows x 6 columns]\n",
      "\n",
      "\n",
      "270    Yes\n",
      "69     Yes\n",
      "289     No\n",
      "294    Yes\n",
      "143     No\n",
      "      ... \n",
      "1      Yes\n",
      "281     No\n",
      "106    Yes\n",
      "227    Yes\n",
      "201     No\n",
      "Name: AHD, Length: 225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the training set:\n",
    "print(X_train)\n",
    "print('\\n')\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak\n",
      "27    66     150   226        0    114      2.6\n",
      "136   62     120   281        2    103      1.4\n",
      "213   52     112   230        0    160      0.0\n",
      "184   63     140   195        0    179      0.0\n",
      "93    63     135   252        2    172      0.0\n",
      "..   ...     ...   ...      ...    ...      ...\n",
      "192   62     138   294        0    106      1.9\n",
      "212   66     178   228        0    165      1.0\n",
      "112   43     132   341        2    136      3.0\n",
      "100   34     118   182        2    174      0.0\n",
      "13    44     120   263        0    173      0.0\n",
      "\n",
      "[76 rows x 6 columns]\n",
      "\n",
      "\n",
      "27      No\n",
      "136    Yes\n",
      "213    Yes\n",
      "184     No\n",
      "93      No\n",
      "      ... \n",
      "192    Yes\n",
      "212    Yes\n",
      "112    Yes\n",
      "100     No\n",
      "13      No\n",
      "Name: AHD, Length: 76, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the testing set:\n",
    "print(X_test)\n",
    "print('\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E\n",
    "Use KNN (with k=3), Decision Tree (with random_state=5), and Logistic Regression Classifiers to predict Heart Disease based on the training/testing datasets that you built in part (d). Then check, compare, and report the accuracy of these 3 classifiers. Which one is the best? Which one is the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using KNN classifier with k = 3 is:  0.6447368421052632\n"
     ]
    }
   ],
   "source": [
    "# Using KNN\n",
    "\n",
    "# Instantiating object of KNeighborsClassifier \"class\" with k=3:\n",
    "k = 3\n",
    "my_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Training the model with train dataset we created using split\n",
    "my_knn.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model with test dataset we created using split\n",
    "\n",
    "# store the predicted values\n",
    "my_knn_y_predict = my_knn.predict(X_test)\n",
    "\n",
    "# Calculating the Accuracy of the prediction \n",
    "\n",
    "# we use accuracy_score function\n",
    "my_knn_accuracy = accuracy_score(y_test, my_knn_y_predict)\n",
    "\n",
    "print('Accuracy using KNN classifier with k = 3 is: ',my_knn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Decision Tree is:  0.618421052631579\n"
     ]
    }
   ],
   "source": [
    "# Using Decision Tree\n",
    "\n",
    "# Instantiating object of DecisionTreeClassifier \"class\" with random_state=5:\n",
    "my_decisiontree = DecisionTreeClassifier(random_state=5)\n",
    "\n",
    "# Training the model with train dataset we created using split\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model with test dataset we created using split\n",
    "\n",
    "# store the predicted values\n",
    "my_decisiontree_y_predict = my_decisiontree.predict(X_test)\n",
    "\n",
    "# Calculating the Accuracy of the prediction \n",
    "\n",
    "# we use accuracy_score function\n",
    "my_decisiontree_accuracy = accuracy_score(y_test, my_decisiontree_y_predict)\n",
    "\n",
    "print('Accuracy using Decision Tree is: ',my_decisiontree_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic Regression is:  0.6710526315789473\n"
     ]
    }
   ],
   "source": [
    "# Using Logistic Regression\n",
    "\n",
    "# Instantiating object of LogisticRegression \"class\"\n",
    "my_logisticregression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Training the model with train dataset we created using split\n",
    "my_logisticregression.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model with test dataset we created using split\n",
    "\n",
    "# store the predicted values\n",
    "my_logisticregression_y_predict = my_logisticregression.predict(X_test)\n",
    "\n",
    "# Calculating the Accuracy of the prediction \n",
    "\n",
    "# we use accuracy_score function\n",
    "my_logisticregression_accuracy = accuracy_score(y_test, my_logisticregression_y_predict)\n",
    "\n",
    "print('Accuracy using Logistic Regression is: ',my_logisticregression_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Algorithm is Logistic Regression with Accuracy Score:  0.6710526315789473\n",
      "Worst Algorithm is Desicion Tree with Accuracy Score:  0.618421052631579\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Algorithm is Logistic Regression with Accuracy Score: \", my_logisticregression_accuracy)\n",
    "print(\"Worst Algorithm is Desicion Tree with Accuracy Score: \", my_decisiontree_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F\n",
    "Now, we want to use the categorical features as well! To this end, we have to perform a feature engineering process called OneHotEncoding for the categorical features. To do this, each categorical feature should be replaced with dummy columns in the feature table (one column for each possible value of a categorical feature), and then encode it in a binary manner such that only one of the dummy columns can take “1” at a time (and zero for the rest). For example, “Gender” can take two values “m” and “f”. Thus, we need to replace this feature (in the feature table) by 2 columns titled “m” and “f”.  Wherever we have a male subject, we can put “1” and ”0” in the columns “m” and “f”.  Wherever we have a female subject, we can put “0” and ”1” in the columns “m” and “f”. (Hint: you will need 4 columns to encode “ChestPain” and 3 columns to encode “Thal”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   f  m\n",
      "0  1  0\n",
      "1  1  0\n",
      "2  1  0\n",
      "3  1  0\n",
      "4  0  1\n",
      "   Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  f  m\n",
      "0   63     145   233        2    150      2.3  1  0\n",
      "1   67     160   286        2    108      1.5  1  0\n",
      "2   67     120   229        2    129      2.6  1  0\n",
      "3   37     130   250        0    187      3.5  1  0\n",
      "4   41     130   204        2    172      1.4  0  1\n"
     ]
    }
   ],
   "source": [
    "# Create a New feature Matrix\n",
    "\n",
    "# create a python list of numerical feature names\n",
    "feature_cols = ['Age','RestBP','Chol','RestECG','MaxHR','Oldpeak']\n",
    "\n",
    "# use the above list to select the features\n",
    "X = df[feature_cols]\n",
    "\n",
    "# Generate Dummy values for Gender\n",
    "dummy_gender = pd.get_dummies(df['Gender'])\n",
    "\n",
    "print(dummy_gender.head())\n",
    "\n",
    "# Concatinate the Dummy values of Gender with feature matrix\n",
    "X = pd.concat([X, dummy_gender], axis=1)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   asymptomatic  nonanginal  nontypical  typical\n",
      "0             0           0           0        1\n",
      "1             1           0           0        0\n",
      "2             1           0           0        0\n",
      "3             0           1           0        0\n",
      "4             0           0           1        0\n",
      "   Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  f  m  asymptomatic  nonanginal  \\\n",
      "0   63     145   233        2    150      2.3  1  0             0           0   \n",
      "1   67     160   286        2    108      1.5  1  0             1           0   \n",
      "2   67     120   229        2    129      2.6  1  0             1           0   \n",
      "3   37     130   250        0    187      3.5  1  0             0           1   \n",
      "4   41     130   204        2    172      1.4  0  1             0           0   \n",
      "\n",
      "   nontypical  typical  \n",
      "0           0        1  \n",
      "1           0        0  \n",
      "2           0        0  \n",
      "3           0        0  \n",
      "4           1        0  \n"
     ]
    }
   ],
   "source": [
    "# Generate Dummy values for ChestPain\n",
    "dummy_chestpain = pd.get_dummies(df['ChestPain'])\n",
    "\n",
    "print(dummy_chestpain.head())\n",
    "# Concatinate the Dummy values of ChestPain with feature matrix\n",
    "X = pd.concat([X, dummy_chestpain], axis=1)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   asymptomatic  nonanginal  nontypical  typical\n",
      "0             0           0           0        1\n",
      "1             1           0           0        0\n",
      "2             1           0           0        0\n",
      "3             0           1           0        0\n",
      "4             0           0           1        0\n",
      "   Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  f  m  asymptomatic  nonanginal  \\\n",
      "0   63     145   233        2    150      2.3  1  0             0           0   \n",
      "1   67     160   286        2    108      1.5  1  0             1           0   \n",
      "2   67     120   229        2    129      2.6  1  0             1           0   \n",
      "3   37     130   250        0    187      3.5  1  0             0           1   \n",
      "4   41     130   204        2    172      1.4  0  1             0           0   \n",
      "\n",
      "   nontypical  typical  fixed  normal  reversable  \n",
      "0           0        1      1       0           0  \n",
      "1           0        0      0       1           0  \n",
      "2           0        0      0       0           1  \n",
      "3           0        0      0       1           0  \n",
      "4           1        0      0       1           0  \n"
     ]
    }
   ],
   "source": [
    "# Generate Dummy values for Thal\n",
    "dummy_thal = pd.get_dummies(df['Thal'])\n",
    "\n",
    "print(dummy_chestpain.head())\n",
    "# Concatinate the Dummy values of Thal with feature matrix\n",
    "X = pd.concat([X, dummy_thal], axis=1)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  f  m  asymptomatic  \\\n",
      "0     63     145   233        2    150      2.3  1  0             0   \n",
      "1     67     160   286        2    108      1.5  1  0             1   \n",
      "2     67     120   229        2    129      2.6  1  0             1   \n",
      "3     37     130   250        0    187      3.5  1  0             0   \n",
      "4     41     130   204        2    172      1.4  0  1             0   \n",
      "..   ...     ...   ...      ...    ...      ... .. ..           ...   \n",
      "296   45     110   264        0    132      1.2  1  0             0   \n",
      "297   68     144   193        0    141      3.4  1  0             1   \n",
      "298   57     130   131        0    115      1.2  1  0             1   \n",
      "299   57     130   236        2    174      0.0  0  1             0   \n",
      "300   38     138   175        0    173      0.0  1  0             0   \n",
      "\n",
      "     nonanginal  nontypical  typical  fixed  normal  reversable  \n",
      "0             0           0        1      1       0           0  \n",
      "1             0           0        0      0       1           0  \n",
      "2             0           0        0      0       0           1  \n",
      "3             1           0        0      0       1           0  \n",
      "4             0           1        0      0       1           0  \n",
      "..          ...         ...      ...    ...     ...         ...  \n",
      "296           0           0        1      0       0           1  \n",
      "297           0           0        0      0       0           1  \n",
      "298           0           0        0      0       0           1  \n",
      "299           0           1        0      0       1           0  \n",
      "300           1           0        0      0       1           0  \n",
      "\n",
      "[301 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# New Feature Matrix\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G\n",
    "Repeat parts (d) and (e) with the new dataset that you built in part (f). How does the prediction accuracy change for each method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly splitting the original dataset into training set and testing set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  f  m  asymptomatic  \\\n",
      "270   46     140   311        0    120      1.8  1  0             1   \n",
      "69    46     150   231        0    147      3.6  1  0             0   \n",
      "289   55     132   342        0    166      1.2  0  1             0   \n",
      "294   59     164   176        2     90      1.0  1  0             1   \n",
      "143   58     105   240        2    154      0.6  1  0             0   \n",
      "..   ...     ...   ...      ...    ...      ... .. ..           ...   \n",
      "1     67     160   286        2    108      1.5  1  0             1   \n",
      "281   35     122   192        0    174      0.0  1  0             0   \n",
      "106   57     128   229        2    150      0.4  1  0             0   \n",
      "227   54     110   206        2    108      0.0  1  0             1   \n",
      "201   57     150   126        0    173      0.2  1  0             0   \n",
      "\n",
      "     nonanginal  nontypical  typical  fixed  normal  reversable  \n",
      "270           0           0        0      0       0           1  \n",
      "69            1           0        0      0       1           0  \n",
      "289           0           1        0      0       1           0  \n",
      "294           0           0        0      1       0           0  \n",
      "143           1           0        0      0       0           1  \n",
      "..          ...         ...      ...    ...     ...         ...  \n",
      "1             0           0        0      0       1           0  \n",
      "281           0           1        0      0       1           0  \n",
      "106           1           0        0      0       0           1  \n",
      "227           0           0        0      0       1           0  \n",
      "201           1           0        0      0       0           1  \n",
      "\n",
      "[225 rows x 15 columns]\n",
      "\n",
      "\n",
      "270    Yes\n",
      "69     Yes\n",
      "289     No\n",
      "294    Yes\n",
      "143     No\n",
      "      ... \n",
      "1      Yes\n",
      "281     No\n",
      "106    Yes\n",
      "227    Yes\n",
      "201     No\n",
      "Name: AHD, Length: 225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the training set:\n",
    "print(X_train)\n",
    "print('\\n')\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  RestBP  Chol  RestECG  MaxHR  Oldpeak  f  m  asymptomatic  \\\n",
      "27    66     150   226        0    114      2.6  0  1             0   \n",
      "136   62     120   281        2    103      1.4  1  0             0   \n",
      "213   52     112   230        0    160      0.0  1  0             1   \n",
      "184   63     140   195        0    179      0.0  0  1             0   \n",
      "93    63     135   252        2    172      0.0  0  1             0   \n",
      "..   ...     ...   ...      ...    ...      ... .. ..           ...   \n",
      "192   62     138   294        0    106      1.9  0  1             1   \n",
      "212   66     178   228        0    165      1.0  0  1             1   \n",
      "112   43     132   341        2    136      3.0  0  1             1   \n",
      "100   34     118   182        2    174      0.0  1  0             0   \n",
      "13    44     120   263        0    173      0.0  1  0             0   \n",
      "\n",
      "     nonanginal  nontypical  typical  fixed  normal  reversable  \n",
      "27            0           0        1      0       1           0  \n",
      "136           0           1        0      0       0           1  \n",
      "213           0           0        0      0       1           0  \n",
      "184           0           1        0      0       1           0  \n",
      "93            1           0        0      0       1           0  \n",
      "..          ...         ...      ...    ...     ...         ...  \n",
      "192           0           0        0      0       1           0  \n",
      "212           0           0        0      0       0           1  \n",
      "112           0           0        0      0       0           1  \n",
      "100           0           0        1      0       1           0  \n",
      "13            0           1        0      0       0           1  \n",
      "\n",
      "[76 rows x 15 columns]\n",
      "\n",
      "\n",
      "27      No\n",
      "136    Yes\n",
      "213    Yes\n",
      "184     No\n",
      "93      No\n",
      "      ... \n",
      "192    Yes\n",
      "212    Yes\n",
      "112    Yes\n",
      "100     No\n",
      "13      No\n",
      "Name: AHD, Length: 76, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print the testing set:\n",
    "print(X_test)\n",
    "print('\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using KNN classifier with k = 3 is:  0.6447368421052632\n"
     ]
    }
   ],
   "source": [
    "# Using KNN\n",
    "\n",
    "# Instantiating object of KNeighborsClassifier \"class\" with k=3:\n",
    "k = 3\n",
    "my_knn1 = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Training the model with train dataset we created using split\n",
    "my_knn1.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model with test dataset we created using split\n",
    "\n",
    "# store the predicted values\n",
    "my_knn1_y_predict = my_knn1.predict(X_test)\n",
    "\n",
    "# Calculating the Accuracy of the prediction \n",
    "\n",
    "# we use accuracy_score function\n",
    "my_knn1_accuracy = accuracy_score(y_test, my_knn1_y_predict)\n",
    "\n",
    "print('Accuracy using KNN classifier with k = 3 is: ',my_knn1_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Decision Tree is:  0.7368421052631579\n"
     ]
    }
   ],
   "source": [
    "# Using Decision Tree\n",
    "\n",
    "# Instantiating object of DecisionTreeClassifier \"class\" with random_state=5:\n",
    "my_decisiontree1 = DecisionTreeClassifier(random_state=5)\n",
    "\n",
    "# Training the model with train dataset we created using split\n",
    "my_decisiontree1.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model with test dataset we created using split\n",
    "\n",
    "# store the predicted values\n",
    "my_decisiontree1_y_predict = my_decisiontree1.predict(X_test)\n",
    "\n",
    "# Calculating the Accuracy of the prediction \n",
    "\n",
    "# we use accuracy_score function\n",
    "my_decisiontree1_accuracy = accuracy_score(y_test, my_decisiontree1_y_predict)\n",
    "\n",
    "print('Accuracy using Decision Tree is: ',my_decisiontree1_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic Regression is:  0.7763157894736842\n"
     ]
    }
   ],
   "source": [
    "# Using Logistic Regression\n",
    "\n",
    "# Instantiating object of LogisticRegression \"class\"\n",
    "my_logisticregression1 = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Training the model with train dataset we created using split\n",
    "my_logisticregression1.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model with test dataset we created using split\n",
    "\n",
    "# store the predicted values\n",
    "my_logisticregression1_y_predict = my_logisticregression1.predict(X_test)\n",
    "\n",
    "# Calculating the Accuracy of the prediction \n",
    "\n",
    "# we use accuracy_score function\n",
    "my_logisticregression1_accuracy = accuracy_score(y_test, my_logisticregression1_y_predict)\n",
    "\n",
    "print('Accuracy using Logistic Regression is: ',my_logisticregression1_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Algorithm is Logistic Regression with Accuracy Score:  0.7763157894736842\n",
      "Worst Algorithm is KNN Classifier with Accuracy Score:  0.6447368421052632\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Algorithm is Logistic Regression with Accuracy Score: \", my_logisticregression1_accuracy)\n",
    "print(\"Worst Algorithm is KNN Classifier with Accuracy Score: \", my_knn1_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H\n",
    "Now, repeat part (e) with the new dataset that you built in part (f), but this time using Cross-Validation. Thus, rather than splitting the dataset into testing and training, use 10-fold Cross-Validation (as we learned in Lab4) to evaluate the classification methods and report the final prediction accuracy. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70967742 0.63333333 0.56666667 0.66666667 0.6        0.5\n",
      " 0.66666667 0.7        0.56666667 0.73333333]\n",
      "Accuracy using KNN Classifier with 10-fold cross validation is:  0.6343010752688172\n"
     ]
    }
   ],
   "source": [
    "# Applying 10-fold cross validation with \"KNN Classifier\":\n",
    "\n",
    "# Instantiating object of KNeighborsClassifier \"class\" with k=3:\n",
    "k = 3\n",
    "my_knn2 = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "knn2_accuracy_list = cross_val_score(my_knn2, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(knn2_accuracy_list)\n",
    "\n",
    "knn2_accuracy_mean = knn2_accuracy_list.mean()\n",
    "\n",
    "print('Accuracy using KNN Classifier with 10-fold cross validation is: ', knn2_accuracy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77419355 0.76666667 0.8        0.76666667 0.8        0.7\n",
      " 0.7        0.66666667 0.7        0.83333333]\n",
      "0.750752688172043\n"
     ]
    }
   ],
   "source": [
    "# Applying 10-fold cross validation with \"Decision Tree\":\n",
    "\n",
    "# Instantiating object of DecisionTree \"class\" \n",
    "my_decisiontree2 = DecisionTreeClassifier(random_state=5)\n",
    "\n",
    "decisiontree2_accuracy_list = cross_val_score(my_decisiontree2, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(decisiontree2_accuracy_list)\n",
    "\n",
    "decisiontree2_accuracy_mean = decisiontree2_accuracy_list.mean()\n",
    "\n",
    "print(decisiontree2_accuracy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77419355 0.8        0.83333333 0.86666667 0.93333333 0.7\n",
      " 0.76666667 0.8        0.8        0.83333333]\n",
      "0.810752688172043\n"
     ]
    }
   ],
   "source": [
    "# Applying 10-fold cross validation with \"Logistic Regression\":\n",
    "\n",
    "# Instantiating object of DecisionTree \"class\" \n",
    "my_logisticregression2 = LogisticRegression(max_iter=10000)\n",
    "\n",
    "logisticregression2_accuracy_list = cross_val_score(my_logisticregression2, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(logisticregression2_accuracy_list)\n",
    "\n",
    "logisticregression2_accuracy_mean = logisticregression2_accuracy_list.mean()\n",
    "\n",
    "print(logisticregression2_accuracy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Algorithm is Logistic Regression with Average Accuracy Score:  0.810752688172043\n",
      "Worst Algorithm is KNN Classifier with Average Accuracy Score:  0.6343010752688172\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Algorithm is Logistic Regression with Average Accuracy Score: \", logisticregression2_accuracy_mean)\n",
    "print(\"Worst Algorithm is KNN Classifier with Average Accuracy Score: \", knn2_accuracy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Summary\n",
    "\n",
    "### E\n",
    "#### Best Algorithm is Logistic Regression with Accuracy Score:  0.6710526315789473\n",
    "#### Worst Algorithm is Desicion Tree with Accuracy Score:  0.618421052631579\n",
    "\n",
    "### G\n",
    "#### Best Algorithm is Logistic Regression with Accuracy Score:  0.7763157894736842\n",
    "#### Worst Algorithm is KNN Classifier with Accuracy Score:  0.6447368421052632\n",
    "\n",
    "### H\n",
    "#### Best Algorithm is Logistic Regression with Average Accuracy Score:  0.810752688172043\n",
    "#### Worst Algorithm is KNN Classifier with Average Accuracy Score:  0.6343010752688172\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
